{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "images.shape: torch.Size([128, 3, 32, 32])\n",
      "labels.shape: torch.Size([128])\n",
      "Epoch 1/250, Loss: 1.8406, Accuracy: 36.62%\n",
      "Epoch 2/250, Loss: 1.7816, Accuracy: 38.60%\n",
      "Epoch 3/250, Loss: 1.7524, Accuracy: 39.74%\n",
      "Epoch 4/250, Loss: 1.7362, Accuracy: 40.43%\n",
      "Epoch 5/250, Loss: 1.7267, Accuracy: 40.70%\n",
      "Epoch 6/250, Loss: 1.7207, Accuracy: 40.84%\n",
      "Epoch 7/250, Loss: 1.7168, Accuracy: 40.99%\n",
      "Epoch 8/250, Loss: 1.7140, Accuracy: 40.91%\n",
      "Epoch 9/250, Loss: 1.7122, Accuracy: 40.86%\n",
      "Epoch 10/250, Loss: 1.7108, Accuracy: 40.88%\n",
      "Epoch 11/250, Loss: 1.7099, Accuracy: 41.02%\n",
      "Epoch 12/250, Loss: 1.7094, Accuracy: 41.08%\n",
      "Epoch 13/250, Loss: 1.7090, Accuracy: 41.18%\n",
      "Epoch 14/250, Loss: 1.7089, Accuracy: 41.28%\n",
      "Epoch 15/250, Loss: 1.7089, Accuracy: 41.16%\n",
      "Epoch 16/250, Loss: 1.7090, Accuracy: 41.13%\n",
      "Epoch 17/250, Loss: 1.7092, Accuracy: 41.16%\n",
      "Epoch 18/250, Loss: 1.7094, Accuracy: 41.20%\n",
      "Epoch 19/250, Loss: 1.7097, Accuracy: 41.18%\n",
      "Epoch 20/250, Loss: 1.7100, Accuracy: 41.21%\n",
      "Epoch 21/250, Loss: 1.7104, Accuracy: 41.17%\n",
      "Epoch 22/250, Loss: 1.7108, Accuracy: 41.14%\n",
      "Epoch 23/250, Loss: 1.7112, Accuracy: 41.27%\n",
      "Epoch 24/250, Loss: 1.7116, Accuracy: 41.24%\n",
      "Epoch 25/250, Loss: 1.7120, Accuracy: 41.18%\n",
      "Epoch 26/250, Loss: 1.7124, Accuracy: 41.15%\n",
      "Epoch 27/250, Loss: 1.7129, Accuracy: 41.13%\n",
      "Epoch 28/250, Loss: 1.7133, Accuracy: 41.16%\n",
      "Epoch 29/250, Loss: 1.7138, Accuracy: 41.18%\n",
      "Epoch 30/250, Loss: 1.7143, Accuracy: 41.15%\n",
      "Epoch 31/250, Loss: 1.7148, Accuracy: 41.20%\n",
      "Epoch 32/250, Loss: 1.7152, Accuracy: 41.18%\n",
      "Epoch 33/250, Loss: 1.7157, Accuracy: 41.21%\n",
      "Epoch 34/250, Loss: 1.7162, Accuracy: 41.19%\n",
      "Epoch 35/250, Loss: 1.7168, Accuracy: 41.16%\n",
      "Epoch 36/250, Loss: 1.7173, Accuracy: 41.08%\n",
      "Epoch 37/250, Loss: 1.7178, Accuracy: 41.08%\n",
      "Epoch 38/250, Loss: 1.7183, Accuracy: 40.96%\n",
      "Epoch 39/250, Loss: 1.7189, Accuracy: 40.92%\n",
      "Epoch 40/250, Loss: 1.7194, Accuracy: 40.89%\n",
      "Epoch 41/250, Loss: 1.7199, Accuracy: 40.83%\n",
      "Epoch 42/250, Loss: 1.7205, Accuracy: 40.89%\n",
      "Epoch 43/250, Loss: 1.7210, Accuracy: 40.90%\n",
      "Epoch 44/250, Loss: 1.7216, Accuracy: 40.87%\n",
      "Epoch 45/250, Loss: 1.7221, Accuracy: 40.87%\n",
      "Epoch 46/250, Loss: 1.7227, Accuracy: 40.81%\n",
      "Epoch 47/250, Loss: 1.7232, Accuracy: 40.84%\n",
      "Epoch 48/250, Loss: 1.7238, Accuracy: 40.81%\n",
      "Epoch 49/250, Loss: 1.7243, Accuracy: 40.82%\n",
      "Epoch 50/250, Loss: 1.7249, Accuracy: 40.76%\n",
      "Epoch 51/250, Loss: 1.7254, Accuracy: 40.69%\n",
      "Epoch 52/250, Loss: 1.7260, Accuracy: 40.64%\n",
      "Epoch 53/250, Loss: 1.7266, Accuracy: 40.61%\n",
      "Epoch 54/250, Loss: 1.7271, Accuracy: 40.59%\n",
      "Epoch 55/250, Loss: 1.7277, Accuracy: 40.52%\n",
      "Epoch 56/250, Loss: 1.7282, Accuracy: 40.52%\n",
      "Epoch 57/250, Loss: 1.7288, Accuracy: 40.50%\n",
      "Epoch 58/250, Loss: 1.7293, Accuracy: 40.46%\n",
      "Epoch 59/250, Loss: 1.7299, Accuracy: 40.47%\n",
      "Epoch 60/250, Loss: 1.7304, Accuracy: 40.45%\n",
      "Epoch 61/250, Loss: 1.7310, Accuracy: 40.48%\n",
      "Epoch 62/250, Loss: 1.7315, Accuracy: 40.40%\n",
      "Epoch 63/250, Loss: 1.7321, Accuracy: 40.37%\n",
      "Epoch 64/250, Loss: 1.7326, Accuracy: 40.43%\n",
      "Epoch 65/250, Loss: 1.7332, Accuracy: 40.45%\n",
      "Epoch 66/250, Loss: 1.7337, Accuracy: 40.41%\n",
      "Epoch 67/250, Loss: 1.7342, Accuracy: 40.43%\n",
      "Epoch 68/250, Loss: 1.7347, Accuracy: 40.41%\n",
      "Epoch 69/250, Loss: 1.7353, Accuracy: 40.43%\n",
      "Epoch 70/250, Loss: 1.7358, Accuracy: 40.46%\n",
      "Epoch 71/250, Loss: 1.7363, Accuracy: 40.46%\n",
      "Epoch 72/250, Loss: 1.7368, Accuracy: 40.46%\n",
      "Epoch 73/250, Loss: 1.7373, Accuracy: 40.42%\n",
      "Epoch 74/250, Loss: 1.7378, Accuracy: 40.43%\n",
      "Epoch 75/250, Loss: 1.7383, Accuracy: 40.42%\n",
      "Epoch 76/250, Loss: 1.7388, Accuracy: 40.37%\n",
      "Epoch 77/250, Loss: 1.7393, Accuracy: 40.36%\n",
      "Epoch 78/250, Loss: 1.7398, Accuracy: 40.32%\n",
      "Epoch 79/250, Loss: 1.7403, Accuracy: 40.32%\n",
      "Epoch 80/250, Loss: 1.7408, Accuracy: 40.31%\n",
      "Epoch 81/250, Loss: 1.7412, Accuracy: 40.30%\n",
      "Epoch 82/250, Loss: 1.7417, Accuracy: 40.22%\n",
      "Epoch 83/250, Loss: 1.7422, Accuracy: 40.13%\n",
      "Epoch 84/250, Loss: 1.7426, Accuracy: 40.09%\n",
      "Epoch 85/250, Loss: 1.7431, Accuracy: 40.07%\n",
      "Epoch 86/250, Loss: 1.7436, Accuracy: 40.02%\n",
      "Epoch 87/250, Loss: 1.7440, Accuracy: 40.03%\n",
      "Epoch 88/250, Loss: 1.7445, Accuracy: 39.99%\n",
      "Epoch 89/250, Loss: 1.7449, Accuracy: 39.96%\n",
      "Epoch 90/250, Loss: 1.7454, Accuracy: 39.99%\n",
      "Epoch 91/250, Loss: 1.7458, Accuracy: 39.92%\n",
      "Epoch 92/250, Loss: 1.7462, Accuracy: 39.90%\n",
      "Epoch 93/250, Loss: 1.7467, Accuracy: 39.94%\n",
      "Epoch 94/250, Loss: 1.7471, Accuracy: 39.95%\n",
      "Epoch 95/250, Loss: 1.7475, Accuracy: 39.99%\n",
      "Epoch 96/250, Loss: 1.7480, Accuracy: 39.94%\n",
      "Epoch 97/250, Loss: 1.7484, Accuracy: 39.93%\n",
      "Epoch 98/250, Loss: 1.7488, Accuracy: 39.91%\n",
      "Epoch 99/250, Loss: 1.7492, Accuracy: 39.90%\n",
      "Epoch 100/250, Loss: 1.7496, Accuracy: 39.85%\n",
      "Epoch 101/250, Loss: 1.7500, Accuracy: 39.86%\n",
      "Epoch 102/250, Loss: 1.7504, Accuracy: 39.84%\n",
      "Epoch 103/250, Loss: 1.7508, Accuracy: 39.78%\n",
      "Epoch 104/250, Loss: 1.7512, Accuracy: 39.79%\n",
      "Epoch 105/250, Loss: 1.7516, Accuracy: 39.72%\n",
      "Epoch 106/250, Loss: 1.7520, Accuracy: 39.69%\n",
      "Epoch 107/250, Loss: 1.7524, Accuracy: 39.72%\n",
      "Epoch 108/250, Loss: 1.7528, Accuracy: 39.72%\n",
      "Epoch 109/250, Loss: 1.7532, Accuracy: 39.70%\n",
      "Epoch 110/250, Loss: 1.7535, Accuracy: 39.67%\n",
      "Epoch 111/250, Loss: 1.7539, Accuracy: 39.65%\n",
      "Epoch 112/250, Loss: 1.7543, Accuracy: 39.64%\n",
      "Epoch 113/250, Loss: 1.7547, Accuracy: 39.61%\n",
      "Epoch 114/250, Loss: 1.7550, Accuracy: 39.64%\n",
      "Epoch 115/250, Loss: 1.7554, Accuracy: 39.65%\n",
      "Epoch 116/250, Loss: 1.7557, Accuracy: 39.67%\n",
      "Epoch 117/250, Loss: 1.7561, Accuracy: 39.66%\n",
      "Epoch 118/250, Loss: 1.7565, Accuracy: 39.66%\n",
      "Epoch 119/250, Loss: 1.7568, Accuracy: 39.68%\n",
      "Epoch 120/250, Loss: 1.7572, Accuracy: 39.68%\n",
      "Epoch 121/250, Loss: 1.7575, Accuracy: 39.66%\n",
      "Epoch 122/250, Loss: 1.7579, Accuracy: 39.67%\n",
      "Epoch 123/250, Loss: 1.7582, Accuracy: 39.64%\n",
      "Epoch 124/250, Loss: 1.7585, Accuracy: 39.64%\n",
      "Epoch 125/250, Loss: 1.7589, Accuracy: 39.63%\n",
      "Epoch 126/250, Loss: 1.7592, Accuracy: 39.63%\n",
      "Epoch 127/250, Loss: 1.7596, Accuracy: 39.61%\n",
      "Epoch 128/250, Loss: 1.7599, Accuracy: 39.62%\n",
      "Epoch 129/250, Loss: 1.7602, Accuracy: 39.62%\n",
      "Epoch 130/250, Loss: 1.7605, Accuracy: 39.57%\n",
      "Epoch 131/250, Loss: 1.7609, Accuracy: 39.57%\n",
      "Epoch 132/250, Loss: 1.7612, Accuracy: 39.55%\n",
      "Epoch 133/250, Loss: 1.7615, Accuracy: 39.55%\n",
      "Epoch 134/250, Loss: 1.7618, Accuracy: 39.53%\n",
      "Epoch 135/250, Loss: 1.7621, Accuracy: 39.54%\n",
      "Epoch 136/250, Loss: 1.7625, Accuracy: 39.56%\n",
      "Epoch 137/250, Loss: 1.7628, Accuracy: 39.55%\n",
      "Epoch 138/250, Loss: 1.7631, Accuracy: 39.54%\n",
      "Epoch 139/250, Loss: 1.7634, Accuracy: 39.52%\n",
      "Epoch 140/250, Loss: 1.7637, Accuracy: 39.47%\n",
      "Epoch 141/250, Loss: 1.7640, Accuracy: 39.44%\n",
      "Epoch 142/250, Loss: 1.7643, Accuracy: 39.39%\n",
      "Epoch 143/250, Loss: 1.7646, Accuracy: 39.39%\n",
      "Epoch 144/250, Loss: 1.7649, Accuracy: 39.43%\n",
      "Epoch 145/250, Loss: 1.7652, Accuracy: 39.43%\n",
      "Epoch 146/250, Loss: 1.7655, Accuracy: 39.40%\n",
      "Epoch 147/250, Loss: 1.7658, Accuracy: 39.39%\n",
      "Epoch 148/250, Loss: 1.7660, Accuracy: 39.42%\n",
      "Epoch 149/250, Loss: 1.7663, Accuracy: 39.40%\n",
      "Epoch 150/250, Loss: 1.7666, Accuracy: 39.38%\n",
      "Epoch 151/250, Loss: 1.7669, Accuracy: 39.38%\n",
      "Epoch 152/250, Loss: 1.7672, Accuracy: 39.42%\n",
      "Epoch 153/250, Loss: 1.7675, Accuracy: 39.44%\n",
      "Epoch 154/250, Loss: 1.7677, Accuracy: 39.42%\n",
      "Epoch 155/250, Loss: 1.7680, Accuracy: 39.39%\n",
      "Epoch 156/250, Loss: 1.7683, Accuracy: 39.37%\n",
      "Epoch 157/250, Loss: 1.7686, Accuracy: 39.37%\n",
      "Epoch 158/250, Loss: 1.7688, Accuracy: 39.36%\n",
      "Epoch 159/250, Loss: 1.7691, Accuracy: 39.36%\n",
      "Epoch 160/250, Loss: 1.7694, Accuracy: 39.35%\n",
      "Epoch 161/250, Loss: 1.7696, Accuracy: 39.33%\n",
      "Epoch 162/250, Loss: 1.7699, Accuracy: 39.33%\n",
      "Epoch 163/250, Loss: 1.7702, Accuracy: 39.31%\n",
      "Epoch 164/250, Loss: 1.7704, Accuracy: 39.29%\n",
      "Epoch 165/250, Loss: 1.7707, Accuracy: 39.30%\n",
      "Epoch 166/250, Loss: 1.7709, Accuracy: 39.30%\n",
      "Epoch 167/250, Loss: 1.7712, Accuracy: 39.29%\n",
      "Epoch 168/250, Loss: 1.7715, Accuracy: 39.31%\n",
      "Epoch 169/250, Loss: 1.7717, Accuracy: 39.32%\n",
      "Epoch 170/250, Loss: 1.7720, Accuracy: 39.33%\n",
      "Epoch 171/250, Loss: 1.7722, Accuracy: 39.30%\n",
      "Epoch 172/250, Loss: 1.7725, Accuracy: 39.29%\n",
      "Epoch 173/250, Loss: 1.7727, Accuracy: 39.27%\n",
      "Epoch 174/250, Loss: 1.7729, Accuracy: 39.27%\n",
      "Epoch 175/250, Loss: 1.7732, Accuracy: 39.23%\n",
      "Epoch 176/250, Loss: 1.7734, Accuracy: 39.23%\n",
      "Epoch 177/250, Loss: 1.7737, Accuracy: 39.20%\n",
      "Epoch 178/250, Loss: 1.7739, Accuracy: 39.18%\n",
      "Epoch 179/250, Loss: 1.7741, Accuracy: 39.12%\n",
      "Epoch 180/250, Loss: 1.7744, Accuracy: 39.12%\n",
      "Epoch 181/250, Loss: 1.7746, Accuracy: 39.12%\n",
      "Epoch 182/250, Loss: 1.7749, Accuracy: 39.13%\n",
      "Epoch 183/250, Loss: 1.7751, Accuracy: 39.10%\n",
      "Epoch 184/250, Loss: 1.7753, Accuracy: 39.09%\n",
      "Epoch 185/250, Loss: 1.7755, Accuracy: 39.05%\n",
      "Epoch 186/250, Loss: 1.7758, Accuracy: 39.03%\n",
      "Epoch 187/250, Loss: 1.7760, Accuracy: 39.06%\n",
      "Epoch 188/250, Loss: 1.7762, Accuracy: 39.05%\n",
      "Epoch 189/250, Loss: 1.7764, Accuracy: 39.04%\n",
      "Epoch 190/250, Loss: 1.7767, Accuracy: 39.03%\n",
      "Epoch 191/250, Loss: 1.7769, Accuracy: 39.03%\n",
      "Epoch 192/250, Loss: 1.7771, Accuracy: 39.02%\n",
      "Epoch 193/250, Loss: 1.7773, Accuracy: 39.01%\n",
      "Epoch 194/250, Loss: 1.7775, Accuracy: 39.01%\n",
      "Epoch 195/250, Loss: 1.7778, Accuracy: 39.02%\n",
      "Epoch 196/250, Loss: 1.7780, Accuracy: 38.97%\n",
      "Epoch 197/250, Loss: 1.7782, Accuracy: 38.98%\n",
      "Epoch 198/250, Loss: 1.7784, Accuracy: 38.97%\n",
      "Epoch 199/250, Loss: 1.7786, Accuracy: 38.97%\n",
      "Epoch 200/250, Loss: 1.7788, Accuracy: 38.97%\n",
      "Epoch 201/250, Loss: 1.7790, Accuracy: 38.97%\n",
      "Epoch 202/250, Loss: 1.7792, Accuracy: 38.96%\n",
      "Epoch 203/250, Loss: 1.7795, Accuracy: 38.92%\n",
      "Epoch 204/250, Loss: 1.7797, Accuracy: 38.90%\n",
      "Epoch 205/250, Loss: 1.7799, Accuracy: 38.90%\n",
      "Epoch 206/250, Loss: 1.7801, Accuracy: 38.90%\n",
      "Epoch 207/250, Loss: 1.7803, Accuracy: 38.90%\n",
      "Epoch 208/250, Loss: 1.7805, Accuracy: 38.90%\n",
      "Epoch 209/250, Loss: 1.7807, Accuracy: 38.88%\n",
      "Epoch 210/250, Loss: 1.7809, Accuracy: 38.89%\n",
      "Epoch 211/250, Loss: 1.7811, Accuracy: 38.86%\n",
      "Epoch 212/250, Loss: 1.7813, Accuracy: 38.87%\n",
      "Epoch 213/250, Loss: 1.7815, Accuracy: 38.85%\n",
      "Epoch 214/250, Loss: 1.7817, Accuracy: 38.87%\n",
      "Epoch 215/250, Loss: 1.7818, Accuracy: 38.84%\n",
      "Epoch 216/250, Loss: 1.7820, Accuracy: 38.85%\n",
      "Epoch 217/250, Loss: 1.7822, Accuracy: 38.84%\n",
      "Epoch 218/250, Loss: 1.7824, Accuracy: 38.87%\n",
      "Epoch 219/250, Loss: 1.7826, Accuracy: 38.88%\n",
      "Epoch 220/250, Loss: 1.7828, Accuracy: 38.88%\n",
      "Epoch 221/250, Loss: 1.7830, Accuracy: 38.89%\n",
      "Epoch 222/250, Loss: 1.7832, Accuracy: 38.87%\n",
      "Epoch 223/250, Loss: 1.7834, Accuracy: 38.88%\n",
      "Epoch 224/250, Loss: 1.7835, Accuracy: 38.90%\n",
      "Epoch 225/250, Loss: 1.7837, Accuracy: 38.89%\n",
      "Epoch 226/250, Loss: 1.7839, Accuracy: 38.86%\n",
      "Epoch 227/250, Loss: 1.7841, Accuracy: 38.86%\n",
      "Epoch 228/250, Loss: 1.7843, Accuracy: 38.87%\n",
      "Epoch 229/250, Loss: 1.7845, Accuracy: 38.88%\n",
      "Epoch 230/250, Loss: 1.7846, Accuracy: 38.87%\n",
      "Epoch 231/250, Loss: 1.7848, Accuracy: 38.85%\n",
      "Epoch 232/250, Loss: 1.7850, Accuracy: 38.86%\n",
      "Epoch 233/250, Loss: 1.7852, Accuracy: 38.85%\n",
      "Epoch 234/250, Loss: 1.7853, Accuracy: 38.86%\n",
      "Epoch 235/250, Loss: 1.7855, Accuracy: 38.86%\n",
      "Epoch 236/250, Loss: 1.7857, Accuracy: 38.87%\n",
      "Epoch 237/250, Loss: 1.7859, Accuracy: 38.86%\n",
      "Epoch 238/250, Loss: 1.7860, Accuracy: 38.85%\n",
      "Epoch 239/250, Loss: 1.7862, Accuracy: 38.84%\n",
      "Epoch 240/250, Loss: 1.7864, Accuracy: 38.83%\n",
      "Epoch 241/250, Loss: 1.7866, Accuracy: 38.83%\n",
      "Epoch 242/250, Loss: 1.7867, Accuracy: 38.83%\n",
      "Epoch 243/250, Loss: 1.7869, Accuracy: 38.81%\n",
      "Epoch 244/250, Loss: 1.7871, Accuracy: 38.80%\n",
      "Epoch 245/250, Loss: 1.7872, Accuracy: 38.80%\n",
      "Epoch 246/250, Loss: 1.7874, Accuracy: 38.80%\n",
      "Epoch 247/250, Loss: 1.7876, Accuracy: 38.79%\n",
      "Epoch 248/250, Loss: 1.7877, Accuracy: 38.79%\n",
      "Epoch 249/250, Loss: 1.7879, Accuracy: 38.79%\n",
      "Epoch 250/250, Loss: 1.7880, Accuracy: 38.79%\n",
      "Time to train: 86.72 seconds\n"
     ]
    }
   ],
   "source": [
    "# We import pytorch and torchvision\n",
    "import torchvision, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))  # Perform normalization on GPU if possible\n",
    "])\n",
    "\n",
    "# We load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "# Charger un dataloader with batch size x\n",
    "def get_data_loader(dataset, batch_size):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "def get_batch_format(data_loader):\n",
    "    images_t, labels_t = next(iter(data_loader))\n",
    "    print('images.shape:', images_t.shape)\n",
    "    print('labels.shape:', labels_t.shape)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, output_dim=10):\n",
    "        super(Network, self).__init__()\n",
    "        self.a = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.b = nn.Linear(hidden_dim1, hidden_dim1)\n",
    "        self.c = nn.Linear(hidden_dim1, hidden_dim1)\n",
    "        self.d = nn.Linear(hidden_dim1, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.permute(0, 2, 3, 1)\n",
    "        x = self.a(x)\n",
    "        x = self.b(x)\n",
    "        x = self.c(x)\n",
    "        x = self.d(x)\n",
    "        return x\n",
    "\n",
    "# Fonction accuracy pour calculer le total des bonnes réponses\n",
    "def accuracy(predictions, labels):\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "    correct = (predicted_labels == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "# Charger dans un DataLoader le data set de train CIFAR10\n",
    "train_loader = get_data_loader(train_dataset, 128) # On charge par batch\n",
    "# On charge le dataset de testto\n",
    "test_loader = get_data_loader(test_dataset, 10000) # On charge tout le dataset de test\n",
    "# Print the format of the batch\n",
    "get_batch_format(train_loader)\n",
    "\n",
    "# On recupère le device\n",
    "deviceGPU = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# On crée une instance de la classe MyClass\n",
    "modelNet = Network(3*32*32, 32, 10)\n",
    "\n",
    "# On envoie le modèle sur le device\n",
    "modelNet = modelNet.to(deviceGPU)\n",
    "\n",
    "# On crée une fonction de loss\n",
    "lossFn = F.cross_entropy\n",
    "learningRate = 0.0001\n",
    "\n",
    "# On crée un optimiseur\n",
    "opt = torch.optim.Adam(modelNet.parameters(), lr=learningRate, weight_decay=0.0001)\n",
    "\n",
    "def off_load_on_gpu(train_loader, test_loader, device):\n",
    "    train_loader_gpu = [(x.to(device).view(x.shape[0], -1), y.to(device)) for x, y in train_loader]\n",
    "    test_loader_gpu = [(x.to(device).view(x.shape[0], -1), y.to(device)) for x, y in test_loader]\n",
    "    return train_loader_gpu, test_loader_gpu\n",
    "\n",
    "def training_cycle(model, train_loader, device):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        #x = x.view(x.shape[0], -1).to(device, non_blocking=True)\n",
    "        #y = y.to(device, non_blocking=True)\n",
    "        preds = model(x)\n",
    "        loss = lossFn(preds, y)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        opt.step()\n",
    "        # Reset gradients to 0\n",
    "        opt.zero_grad()\n",
    "    \n",
    "def validation_cycle(model, test_loader, epoch, num_epochs, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            #x = x.view(x.shape[0], -1).to(device, non_blocking=True)\n",
    "            #y = y.to(device, non_blocking=True)\n",
    "            preds = model(x)\n",
    "            loss = lossFn(preds, y)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += accuracy(preds, y)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_acc = total_correct / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc * 100:.2f}%\")\n",
    "\n",
    "def fit_one_cycle(model, train_loader, test_loader, epoch, num_epochs, device): \n",
    "    # Training\n",
    "    training_cycle(model, train_loader, device)\n",
    "    \n",
    "    # Validation\n",
    "    validation_cycle(model, test_loader, epoch, num_epochs, device)\n",
    "    \n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "# Time to train\n",
    "time_to_train = time.time()\n",
    "\n",
    "# On charge les données sur le GPU\n",
    "train_loader_gpu, test_loader_gpu = off_load_on_gpu(train_loader, test_loader, deviceGPU)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    fit_one_cycle(modelNet, train_loader_gpu, test_loader_gpu, epoch, num_epochs, deviceGPU)\n",
    "\n",
    "time_to_train = time.time() - time_to_train\n",
    "print(f\"Time to train: {time_to_train:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
