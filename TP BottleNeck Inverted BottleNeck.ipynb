{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Normal\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torchvision, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define the transform\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))  # Normalize on GPU if possible\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "# Function to create DataLoader\n",
    "def get_data_loader(dataset, batch_size):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Display batch format\n",
    "def get_batch_format(data_loader):\n",
    "    images_t, labels_t = next(iter(data_loader))\n",
    "    print('images.shape:', images_t.shape)\n",
    "    print('labels.shape:', labels_t.shape)\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy(predictions, labels):\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "    correct = (predicted_labels == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "# Load DataLoaders\n",
    "train_loader = get_data_loader(train_dataset, 128)\n",
    "test_loader = get_data_loader(test_dataset, 10000)\n",
    "get_batch_format(train_loader)\n",
    "\n",
    "# Set device\n",
    "deviceGPU = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Loss function and learning rate\n",
    "lossFn = F.cross_entropy\n",
    "learningRate = 0.0001\n",
    "\n",
    "# Define the custom MyConv class\n",
    "class MyConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation=nn.ReLU(inplace=False)):\n",
    "        super(MyConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "# Define the ConvNetResidual model using MyConv\n",
    "class ConvNetResidual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetResidual, self).__init__()\n",
    "        self.conv1 = MyConv(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = MyConv(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = MyConv(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "        # Residual connections\n",
    "        self.res_conv1 = nn.Conv2d(3, 32, kernel_size=1, stride=1, padding=0)\n",
    "        self.res_conv2 = nn.Conv2d(32, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.res_conv3 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.res_conv1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = x + residual\n",
    "        x = self.pool(x)\n",
    "\n",
    "        residual = self.res_conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + residual\n",
    "        x = self.pool(x)\n",
    "\n",
    "        residual = self.res_conv3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + residual\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "modelNet = ConvNetResidual().to(deviceGPU)\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(modelNet.parameters(), lr=learningRate, weight_decay=0.0001)\n",
    "\n",
    "# Offload data to GPU\n",
    "def off_load_on_gpu(train_loader, test_loader, device):\n",
    "    train_loader_gpu = [(x.to(device), y.to(device)) for x, y in train_loader]\n",
    "    test_loader_gpu = [(x.to(device), y.to(device)) for x, y in test_loader]\n",
    "    return train_loader_gpu, test_loader_gpu\n",
    "\n",
    "# Training cycle\n",
    "def training_cycle(model, train_loader, device):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        preds = model(x)\n",
    "        loss = lossFn(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "# Validation cycle\n",
    "def validation_cycle(model, test_loader, epoch, num_epochs, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            preds = model(x)\n",
    "            loss = lossFn(preds, y)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += accuracy(preds, y)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_acc = total_correct / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc * 100:.2f}%\")\n",
    "\n",
    "# Training and validation for one epoch\n",
    "def fit_one_cycle(model, train_loader, test_loader, epoch, num_epochs, device): \n",
    "    training_cycle(model, train_loader, device)\n",
    "    validation_cycle(model, test_loader, epoch, num_epochs, device)\n",
    "\n",
    "# Load data onto GPU\n",
    "train_loader_gpu, test_loader_gpu = off_load_on_gpu(train_loader, test_loader, deviceGPU)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "time_to_train = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    fit_one_cycle(modelNet, train_loader_gpu, test_loader_gpu, epoch, num_epochs, deviceGPU)\n",
    "\n",
    "time_to_train = time.time() - time_to_train\n",
    "print(f\"Time to train: {time_to_train:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Bottleneck Block: Reduces, maintains, and restores the information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/50, Train Loss: 1.6378, Val Loss: 1.4811, Val Accuracy: 47.55%\n",
      "Epoch 2/50, Train Loss: 1.2066, Val Loss: 1.2412, Val Accuracy: 56.93%\n",
      "Epoch 3/50, Train Loss: 0.9693, Val Loss: 1.0040, Val Accuracy: 65.24%\n",
      "Epoch 4/50, Train Loss: 0.8200, Val Loss: 0.8809, Val Accuracy: 69.58%\n",
      "Epoch 5/50, Train Loss: 0.7159, Val Loss: 0.8805, Val Accuracy: 70.30%\n",
      "Epoch 6/50, Train Loss: 0.6284, Val Loss: 0.8017, Val Accuracy: 72.64%\n",
      "Epoch 7/50, Train Loss: 0.5611, Val Loss: 0.7893, Val Accuracy: 74.37%\n",
      "Epoch 8/50, Train Loss: 0.5005, Val Loss: 0.7202, Val Accuracy: 75.80%\n",
      "Epoch 9/50, Train Loss: 0.4475, Val Loss: 0.7595, Val Accuracy: 74.35%\n",
      "Epoch 10/50, Train Loss: 0.4006, Val Loss: 0.7611, Val Accuracy: 75.84%\n",
      "Epoch 11/50, Train Loss: 0.3606, Val Loss: 1.0420, Val Accuracy: 68.01%\n",
      "Epoch 12/50, Train Loss: 0.3221, Val Loss: 0.8220, Val Accuracy: 74.86%\n",
      "Epoch 13/50, Train Loss: 0.2896, Val Loss: 0.7572, Val Accuracy: 76.66%\n",
      "Epoch 14/50, Train Loss: 0.2587, Val Loss: 0.8240, Val Accuracy: 76.40%\n",
      "Epoch 15/50, Train Loss: 0.2370, Val Loss: 0.8160, Val Accuracy: 76.67%\n",
      "Epoch 16/50, Train Loss: 0.2112, Val Loss: 0.8565, Val Accuracy: 75.47%\n",
      "Epoch 17/50, Train Loss: 0.1965, Val Loss: 0.8301, Val Accuracy: 76.97%\n",
      "Epoch 18/50, Train Loss: 0.1714, Val Loss: 0.8886, Val Accuracy: 76.66%\n",
      "Epoch 19/50, Train Loss: 0.1606, Val Loss: 0.8572, Val Accuracy: 76.66%\n"
     ]
    }
   ],
   "source": [
    "import torchvision, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Transformations pour CIFAR-10\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Chargement des datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# DÃ©finition du device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Classe Bottleneck\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        mid_channels = out_channels // 4\n",
    "        \n",
    "        self.reduce = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.reduce_bn = nn.BatchNorm2d(mid_channels)\n",
    "        \n",
    "        self.maintain = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.maintain_bn = nn.BatchNorm2d(mid_channels)\n",
    "        \n",
    "        self.expand = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.expand_bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.residual_connection = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        x = self.activation(self.reduce_bn(self.reduce(x)))\n",
    "        x = self.activation(self.maintain_bn(self.maintain(x)))\n",
    "        x = self.expand_bn(self.expand(x))\n",
    "        \n",
    "        if self.residual_connection:\n",
    "            x += identity\n",
    "        return self.activation(x)\n",
    "\n",
    "# Stacking de Bottlenecks\n",
    "class BottleneckStack(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, stride=1):\n",
    "        super(BottleneckStack, self).__init__()\n",
    "        layers = [Bottleneck(in_channels, out_channels, stride)]\n",
    "        layers += [Bottleneck(out_channels, out_channels, stride=1) for _ in range(num_blocks - 1)]\n",
    "        self.stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "# ModÃ¨le principal\n",
    "class CIFAR10BottleneckNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10BottleneckNet, self).__init__()\n",
    "        self.init_conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(64)\n",
    "        self.init_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.block1 = BottleneckStack(64, 128, num_blocks=3, stride=2)\n",
    "        self.block2 = BottleneckStack(128, 256, num_blocks=4, stride=2)\n",
    "        self.block3 = BottleneckStack(256, 512, num_blocks=6, stride=2)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_relu(self.init_bn(self.init_conv(x)))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modÃ¨le\n",
    "model = CIFAR10BottleneckNet().to(device)\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Boucle d'entraÃ®nement\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Boucle de validation\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "# EntraÃ®nement sur plusieurs Ã©poques\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accuracy = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Inverted Bottleneck Block: Expands, maintains (via depthwise separable convolution), and reduces back.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Transformations pour CIFAR-10\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Chargement des datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# DÃ©finition du device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Classe Inverted Bottleneck\n",
    "class InvertedBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor, stride):\n",
    "        super(InvertedBottleneck, self).__init__()\n",
    "        mid_channels = in_channels * expansion_factor\n",
    "\n",
    "        self.use_residual = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.expand = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.expand_bn = nn.BatchNorm2d(mid_channels)\n",
    "\n",
    "        self.depthwise = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, groups=mid_channels, bias=False)\n",
    "        self.depthwise_bn = nn.BatchNorm2d(mid_channels)\n",
    "\n",
    "        self.project = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.project_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.activation(self.expand_bn(self.expand(x)))\n",
    "        x = self.activation(self.depthwise_bn(self.depthwise(x)))\n",
    "        x = self.project_bn(self.project(x))\n",
    "\n",
    "        if self.use_residual:\n",
    "            x += identity\n",
    "\n",
    "        return x\n",
    "\n",
    "# Stacking de Inverted Bottlenecks\n",
    "class InvertedBottleneckStack(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, expansion_factor, stride):\n",
    "        super(InvertedBottleneckStack, self).__init__()\n",
    "        layers = [InvertedBottleneck(in_channels, out_channels, expansion_factor, stride)]\n",
    "        layers += [InvertedBottleneck(out_channels, out_channels, expansion_factor, stride=1) for _ in range(num_blocks - 1)]\n",
    "        self.stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "# ModÃ¨le principal avec Inverted Bottlenecks\n",
    "class CIFAR10InvertedBottleneckNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10InvertedBottleneckNet, self).__init__()\n",
    "        self.init_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(32)\n",
    "        self.init_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.block1 = InvertedBottleneckStack(32, 64, num_blocks=2, expansion_factor=6, stride=2)\n",
    "        self.block2 = InvertedBottleneckStack(64, 128, num_blocks=3, expansion_factor=6, stride=2)\n",
    "        self.block3 = InvertedBottleneckStack(128, 256, num_blocks=4, expansion_factor=6, stride=2)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_relu(self.init_bn(self.init_conv(x)))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modÃ¨le\n",
    "model = CIFAR10InvertedBottleneckNet().to(device)\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Boucle d'entraÃ®nement\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Boucle de validation\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "# EntraÃ®nement sur plusieurs Ã©poques\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accuracy = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
